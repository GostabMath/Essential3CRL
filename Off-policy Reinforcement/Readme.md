# Learning Policy with 3CRL

### Section Outline

>This module is the central focus of 3CRL as it gives a big picture of the most recent off-policy/offline reinforcement leanring under different frameworks.
Albeit that conventioanl methods in Causal Inference arises in most situations where experiments are not feasibly scheduled, it still has some affinities with
offline reinforcement learning. In the paper [Causal Machine Learning|Causal Machine Learning:A Survey and Open Problems](https://arxiv.org/pdf/2206.15475.pdf)
we have highlighted, the term " offline" corresponds to many conventional Causal Inference questions: we get a data realization or sample from an
unknown distribution. Following what we have investigated in *Fundamental*, two unknown distribution or policies could be easily defined as the full-control distrbution and full-treated distribution or policy. Here I just use a oversimplified example to connect offline reinforcement learning and conventional causal inference. In this section, we will present a relatively thorough investigation onto offline reinforcement learning.
<hr>

**Overview of Offline Reinforcement Learning**

|   Field     | Publication   | Year          |
|-------------| ------------- | ------------- |
||Offline Reinforcement Learning: Tutorial, Review,
and Perspectives on Open Problems|[2020](https://arxiv.org/pdf/2005.01643.pdf)|