# Learning Policy with 3CRL

### Section Outline

>This module is the central focus of 3CRL as it gives a big picture of the most recent off-policy/offline reinforcement leanring under different frameworks.
Albeit that conventioanl methods in Causal Inference arises in most situations where experiments are not feasibly scheduled, it still has some affinities with
offline reinforcement learning. In the paper [Causal Machine Learning|Causal Machine Learning:A Survey and Open Problems](https://arxiv.org/pdf/2206.15475.pdf)
we have highlighted, the term " offline" corresponds to many conventional Causal Inference questions: we get a data realization or sample from an
unknown distribution. Following what we have investigated in *Fundamental*, two unknown distribution or policies could be easily defined as the full-control
distrbution and full-treated distribution
or policy 
<hr>
